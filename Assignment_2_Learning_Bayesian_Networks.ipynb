{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmprovan/CS4705/blob/main/Assignment_2_Learning_Bayesian_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this notebook is to gain some practice in learning graphical models. Your goal is to:\n",
        "\n",
        "1.   load the Breast Cancer (categorical) data set: https://archive.ics.uci.edu/ml/datasets/breast+cancer\n",
        "2.   keep the last 20% of the data for testing\n",
        "3.   compare the performance of 3 learned models on the test data: naive Bayes, tree-structured BN (using the Chow-Liu algorithm), and BN\n",
        "\n",
        "Below I provide some code fragments to assist with this task.\n",
        "\n",
        "**Marks:**\n",
        "*   70%: successful learning of 3 models\n",
        "*   30%: critical discussion of the reasons for any differences in predictive accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "ISw4_Z_R1d_8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr7Z3IA9npXC"
      },
      "source": [
        "!pip install  pgmpy\n",
        "!pip install pandas\n",
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First learn a naive Bayes model"
      ],
      "metadata": {
        "id": "RE3ITHto7e5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pgmpy.models import NaiveBayes\n",
        "# create the structure manually to create model_struct\n",
        "\n",
        "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
        "\n",
        "mle = MaximumLikelihoodEstimator(model=model_struct, data= ****)"
      ],
      "metadata": {
        "id": "NhVCpGKUM3q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, learn a tree-structured model"
      ],
      "metadata": {
        "id": "m0ADjdH07jFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pgmpy.estimators import TreeSearch\n",
        "\n",
        "# learn graph structure\n",
        "est = TreeSearch(df_data, root_node=\"A\")\n",
        "dag = est.estimate(estimator_type=\"chow-liu\")\n",
        "\n",
        "from pgmpy.estimators import BayesianEstimator\n",
        "\n",
        "# there are many choices of parametrization, here is one example\n",
        "model = BayesianNetwork(dag.edges())\n",
        "model.fit(\n",
        "    data, estimator=BayesianEstimator, prior_type=\"dirichlet\", pseudo_counts=0.1\n",
        ")\n",
        "model.get_cpds()"
      ],
      "metadata": {
        "id": "2W52lkXeMqqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally learn a Bayesian network. **First learn the structure, and then the parameters.**"
      ],
      "metadata": {
        "id": "3UwlIZRx7nRc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzNfOBdPiINs"
      },
      "source": [
        "# **Learning Bayesian Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haooIWMtiSQP"
      },
      "source": [
        "We now want to learn a Bayesian network, given a set of sample data. Learning a Bayesian network can be split into two problems:\n",
        "\n",
        " **Structure learning**: Given a set of data samples, estimate a DAG that captures the dependencies between the variables.\n",
        "\n",
        "  **Parameter learning**: Given a set of data samples and a DAG that captures the dependencies between the variables, estimate the (conditional) probability distributions of the individual variables.\n",
        "\n",
        "\n",
        "Methods for doing this include:\n",
        "\n",
        "Structure learning for discrete, fully observed networks:\n",
        "    \n",
        "*    Score-based structure estimation (BIC/BDeu/K2 score; exhaustive search, hill climb/tabu search)\n",
        "*   Constraint-based structure estimation (PC)\n",
        "\n",
        "Parameter learning for discrete nodes:\n",
        "\n",
        "*   Maximum Likelihood Estimation\n",
        "*   Bayesian Estimation\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WglsabviF7O"
      },
      "source": [
        "**Structure Learning**\n",
        "\n",
        "You can use MLE or Bayesian estimation methods.\n",
        "\n",
        "MLE State counts\n",
        "\n",
        "To make sense of the given data, we can start by counting how often each state of the variable occurs. If the variable is dependent on parents, the counts are done conditionally on the parents states, i.e. for separately for each parent configuration:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24DHWylrkm4q"
      },
      "source": [
        "**Bayesian Parameter Estimation**\n",
        "\n",
        "\n",
        "The Bayesian Parameter Estimator starts with already existing prior CPDs, that express our beliefs about the variables before the data was observed. Those \"priors\" are then updated, using the state counts from the observed data. \n",
        "\n",
        "One can think of the priors as consisting in pseudo state counts, that are added to the actual counts before normalization. Unless one wants to encode specific beliefs about the distributions of the variables, one commonly chooses uniform priors, i.e. ones that deem all states equiprobable.\n",
        "\n",
        "A very simple prior is the so-called K2 prior, which simply adds 1 to the count of every single state. A somewhat more sensible choice of prior is BDeu (Bayesian Dirichlet equivalent uniform prior). For BDeu we need to specify an equivalent sample size N and then the pseudo-counts are the equivalent of having observed N uniform samples of each variable (and each parent configuration). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaeRKm7qj-SS"
      },
      "source": [
        "**Maximum Likelihood Estimation**\n",
        "\n",
        "\n",
        "A natural estimate for the CPDs is to simply use the relative frequencies, with which the variable states have occured. \n",
        "\n",
        "This approach is Maximum Likelihood Estimation (MLE). According to MLE, we should fill the CPDs in such a way, that $P(\\text{data}|\\text{model})$ is maximal. This is achieved when using the relative frequencies.  pgmpy supports MLE as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njSaSDoFkcvK"
      },
      "source": [
        "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
        "mle = MaximumLikelihoodEstimator(model, data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0ey1LYUkeZs"
      },
      "source": [
        "\n",
        "mle.estimate_cpd(variable) computes the state counts and divides each cell by the (conditional) sample size. The mle.get_parameters()-method returns a list of CPDs for all variable of the model.\n",
        "\n",
        "The built-in fit()-method of BayesianModel provides more convenient access to parameter estimators:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaWwb3bjkh5f"
      },
      "source": [
        "# Calibrate all CPDs of `model` using MLE:\n",
        "model.fit(data, estimator=MaximumLikelihoodEstimator)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T9pPn62k0OQ"
      },
      "source": [
        "# **Structure Learning**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "To learn model structure (a DAG) from a data set, there are two broad techniques:\n",
        "\n",
        "*   score-based structure learning\n",
        "*   constraint-based structure learning\n",
        "\n",
        "In this assignment focus on the score-based approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N3IL_H2lF8m"
      },
      "source": [
        "# **Score-based Structure Learning**\n",
        "\n",
        "\n",
        "This approach construes model selection as an optimization task. It has two building blocks:\n",
        "\n",
        "A scoring function $s_D\\colon M \\to \\mathbb R$ that maps models to a numerical score, based on how well they fit to a given data set $D$.\n",
        "A search strategy to traverse the search space of possible models $M$ and select a model with optimal score.\n",
        "\n",
        "\n",
        "**Scoring functions**\n",
        "\n",
        "\n",
        "Commonly used scores to measure the fit between model and data are Bayesian Dirichlet scores such as BDeu or K2 and the Bayesian Information Criterion (BIC, also called MDL). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jheNi_tlOVL"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pgmpy.estimators import K2Score, BicScore\n",
        "from pgmpy.models import BayesianModel\n",
        "\n",
        "\n",
        "\n",
        "k2 = K2Score(data)\n",
        "bic = BicScore(data)\n",
        "\n",
        "#GENERATE MODELS HERE\n",
        "model1 = BayesianModel(...)\n",
        "model2 = BayesianModel(...)\n",
        "\n",
        "#you can compare the performance of the different scoring methods\n",
        "\n",
        "print(k2.score(model1))\n",
        "print(bic.score(model1))\n",
        "\n",
        "\n",
        "print(k2.score(model2))\n",
        "print(bic.score(model2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KEzCraWlUPg"
      },
      "source": [
        "\n",
        "**Search strategies**\n",
        "\n",
        "\n",
        "The search space of DAGs is super-exponential in the number of variables and the above scoring functions allow for local maxima. The first property makes exhaustive search intractable for all but very small networks, the second prohibits efficient local optimization algorithms to always find the optimal structure. Thus, identifiying the ideal structure is often not tractable. Despite these bad news, heuristic search strategies often yields good results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCj58MtblaLe"
      },
      "source": [
        "Heuristic search: HillClimbSearch implements a greedy local search that starts from the DAG start (default: disconnected DAG) and proceeds by iteratively performing single-edge manipulations that maximally increase the score. The search terminates once a local maximum is found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5smiEZ1lc1y"
      },
      "source": [
        "from pgmpy.estimators import HillClimbSearch\n",
        "\n",
        "est = HillClimbSearch(data)\n",
        "best_model = est.estimate(scoring_method=BicScore(data))\n",
        "print(best_model.edges())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ehug-m1ksU_"
      },
      "source": [
        "from pgmpy.estimators import BayesianEstimator\n",
        "est = BayesianEstimator(model, data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rODmDHiku43"
      },
      "source": [
        "\n",
        "The estimated values in the CPDs are now more conservative. \n",
        "\n",
        "BayesianEstimator, too, can be used via the fit()-method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO8azF9pkv8H"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.estimators import BayesianEstimator\n",
        "\n",
        "\n",
        "##  model = BayesianModel(****)\n",
        "\n",
        "model.fit(data, estimator=BayesianEstimator, prior_type=\"BDeu\") # default equivalent_sample_size=5\n",
        "for cpd in model.get_cpds():\n",
        "    print(cpd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVqZN1ZVmdi7"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# **Discussion**\n",
        "\n",
        "Please critically compare the performance of the 3 different models."
      ]
    }
  ]
}